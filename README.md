# DistDataParallel-Pytorch
Distributed Data Parallel using Pytorch in conjuction with MPI-like communication frameworks like Gloo. 

## Repo Target

1. Deploy and configure distributed ML training frameworks in Cloudlab.
2. Implement distributed training application for image classification with Pytorch.
3. Investigate the trade-off between different methods of performing distributed training.
4. Understand how ML frameworks interact with collective communication frameworks.
